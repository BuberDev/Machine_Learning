{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ3CrHzkrcX87PRU0uBzCX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BuberDev/Machine_Learning/blob/main/MoE_v_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqs7IzK8TB0s",
        "outputId": "536c7ffa-fbcf-41bf-aefa-4966a57a625b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "!pip install transformers\n",
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "url = \"http://ipfs.opentensor.ai/api/v0/object/get?arg=Qme2dawBzozFGtKWX73fh5fmB8NJD7TRS2XSWKhJB4WbJd\"\n",
        "response = requests.post(url)\n",
        "text = response.content.decode('utf-8')"
      ],
      "metadata": {
        "id": "VeXhBm4wUMlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')\n",
        "encoded_text = tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "dCW1OI5hTlej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(encoded_text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "XkERH21Udls-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Mixture of Experts model\n",
        "class MoEModel(nn.Module):\n",
        "    def __init__(self, num_experts, expert_model):\n",
        "        super(MoEModel, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.experts = nn.ModuleList([expert_model() for i in range(num_experts)])\n",
        "        self.gate = nn.Linear(1024, num_experts)\n",
        "        \n",
        "    def forward(self, input_ids):\n",
        "        expert_outputs = []\n",
        "        for i in range(self.num_experts):\n",
        "            expert_outputs.append(self.experts[i](input_ids))\n",
        "        expert_outputs = torch.stack(expert_outputs, dim=1)\n",
        "        gate_outputs = self.gate(input_ids)\n",
        "        gate_outputs = torch.softmax(gate_outputs, dim=1)\n",
        "        mixed_output = torch.sum(expert_outputs * gate_outputs.unsqueeze(2), dim=1)\n",
        "        return mixed_output"
      ],
      "metadata": {
        "id": "NzN8cU6yUT-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE6Up-ztYGo4",
        "outputId": "321a765d-9084-4771-ddb7-f0d8815e2660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 21 14:45:19 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0    33W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Fgx_VSaYJd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the MoE model\n",
        "from transformers import GPTNeoConfig, GPTNeoForCausalLM, AutoTokenizer, AutoModel\n",
        "!pip install moe\n",
        "\n",
        "# Inicjalizacja konfiguracji modelu\n",
        "#model_config = GPTNeoConfig.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "\n",
        "# Inicjalizacja modelu eksperta\n",
        "#expert_model = GPTNeoForCausalLM(model_config)\n",
        "\n",
        "#Alternatywny kod\n",
        "expert_model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "expert_model = AutoModel.from_pretrained(expert_model_name)\n",
        "\n",
        "# generowanie sekwencji z u≈ºyciem modelu MoEModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(expert_model_name)\n",
        "inputs_embeds  = tokenizer(encoded_text, return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# Inicjalizacja modelu MoE\n",
        "num_experts = 4\n",
        "model = MoEModel(num_experts, expert_model=expert_model, input_ids=inputs_embeds)\n",
        "\n",
        "outputs = model(inputs_embeds=inputs_embeds)\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "ykpA5MoaUX2f",
        "outputId": "db67bb8f-aa39-4336-b5be-0af233cc7207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: moe in /usr/local/lib/python3.9/dist-packages (2.1.0)\n",
            "Requirement already satisfied: SQLAlchemy<2.0.0,>=1.4.15 in /usr/local/lib/python3.9/dist-packages (from moe) (1.4.46)\n",
            "Requirement already satisfied: musicbrainzngs<0.8.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from moe) (0.7.1)\n",
            "Requirement already satisfied: pluggy<0.14.0,>=0.13.1 in /usr/local/lib/python3.9/dist-packages (from moe) (0.13.1)\n",
            "Requirement already satisfied: questionary<2.0.0,>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from moe) (1.10.0)\n",
            "Requirement already satisfied: dynaconf<4.0.0,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from moe) (3.1.12)\n",
            "Requirement already satisfied: mediafile<0.12.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from moe) (0.11.0)\n",
            "Requirement already satisfied: rich<13.0.0,>=12.5.1 in /usr/local/lib/python3.9/dist-packages (from moe) (12.6.0)\n",
            "Requirement already satisfied: alembic<2.0.0,>=1.4.2 in /usr/local/lib/python3.9/dist-packages (from moe) (1.10.2)\n",
            "Requirement already satisfied: Unidecode<2.0.0,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from moe) (1.3.6)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic<2.0.0,>=1.4.2->moe) (4.5.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.9/dist-packages (from alembic<2.0.0,>=1.4.2->moe) (1.2.4)\n",
            "Requirement already satisfied: mutagen>=1.46 in /usr/local/lib/python3.9/dist-packages (from mediafile<0.12.0,>=0.11.0->moe) (1.46.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.9/dist-packages (from mediafile<0.12.0,>=0.11.0->moe) (1.15.0)\n",
            "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from questionary<2.0.0,>=1.9.0->moe) (2.0.10)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0.0,>=12.5.1->moe) (2.6.1)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from rich<13.0.0,>=12.5.1->moe) (0.9.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from SQLAlchemy<2.0.0,>=1.4.15->moe) (2.0.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary<2.0.0,>=1.9.0->moe) (0.2.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.9/dist-packages (from Mako->alembic<2.0.0,>=1.4.2->moe) (2.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPTNeoModel were not initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B and are newly initialized: ['transformer.h.7.attn.attention.bias', 'transformer.h.13.attn.attention.bias', 'transformer.h.11.attn.attention.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.19.attn.attention.bias', 'transformer.h.17.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.5.attn.attention.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d395133996d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Inicjalizacja modelu MoE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mnum_experts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMoEModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_experts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpert_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-6f0b39ee1e76>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_experts, expert_model)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMoEModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_experts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_experts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexpert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_experts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_experts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-6f0b39ee1e76>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMoEModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_experts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_experts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexpert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_experts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_experts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You have to specify either input_ids or inputs_embeds"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "W linii input_ids = tokenizer.encode(encoded_text, return_tensors=\"pt\") powinno byƒá text zamiast encoded_text, poniewa≈º encoded_text jest ju≈º zakodowanym tekstem.\n",
        "\n",
        "W linii inputs_embeds = tokenizer(encoded_text, return_tensors=\"pt\")[\"input_ids\"] powinno byƒá input_ids zamiast encoded_text.\n",
        "\n",
        "W linii outputs = model(inputs_embeds=inputs_embeds) powinno byƒá outputs = model(inputs_embeds) zamiast inputs_embeds=inputs_embeds.\n",
        "\n",
        "W pƒôtli treningowej powinno byƒá inputs = input_ids[:, i:i+1024] zamiast inputs = torch.tensor(encoded_text[i:i+1024]).unsqueeze(0).\n",
        "\n",
        "W pƒôtli treningowej powinno byƒá optimizer.zero_grad() przed loss.backward().\n",
        "\n",
        "W pƒôtli treningowej nale≈ºy dodaƒá total_loss += loss.item().\n",
        "\n",
        "W pƒôtli treningowej powinno byƒá inputs[:, :-1] zamiast labels[:, :-1].\n",
        "\n",
        "W pƒôtli treningowej powinno byƒá labels[:, 1:] zamiast labels[:, :-1].\n",
        "\n",
        "Kod nie zawiera czƒô≈õci kodu potrzebnej do uruchomienia modelu w trybie predykcji, dlatego kod ten jest tylko czƒô≈õciowo kompletny."
      ],
      "metadata": {
        "id": "3DojJnKPsuCt"
      }
    }
  ]
}